{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP6GQNwnCrwz"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/borongzhang/back_projection_diffusion.git@main\n",
      "  Cloning https://github.com/borongzhang/back_projection_diffusion.git (to revision main) to /tmp/pip-req-build-93f_usgy\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/borongzhang/back_projection_diffusion.git /tmp/pip-req-build-93f_usgy\n",
      "  Resolved https://github.com/borongzhang/back_projection_diffusion.git to commit 2158415f5f3de6d93a13d17fd2e0f097f4660e94\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/bzhang388/miniconda3/envs/jaxflax_test/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/bzhang388/miniconda3/envs/jaxflax_test/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/bzhang388/miniconda3/envs/jaxflax_test/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 143, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-qvi33bht/overlay/lib/python3.11/site-packages/flit_core/buildapi.py\", line 31, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     module = Module(info.module, Path.cwd())\n",
      "  \u001b[31m   \u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-qvi33bht/overlay/lib/python3.11/site-packages/flit_core/common.py\", line 59, in __init__\n",
      "  \u001b[31m   \u001b[0m     raise ValueError(\"No file/folder found for module {}\".format(name))\n",
      "  \u001b[31m   \u001b[0m ValueError: No file/folder found for module back_projection_diffusion\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/borongzhang/back_projection_diffusion.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the current working directory to the \"src\" folder, located one level up from the current directory.\n",
    "# Note: This command can only be run once in a Jupyter notebook, as it permanently changes the working directory.\n",
    "%cd ../src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDKhSAGaCrk2"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import fstars \n",
    "import fstar_cnn\n",
    "\n",
    "import functools\n",
    "import os\n",
    "from clu import metric_writers\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "import h5py\n",
    "import natsort\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import geometric_transform\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from swirl_dynamics import templates\n",
    "from swirl_dynamics.lib import diffusion as dfn_lib\n",
    "from swirl_dynamics.lib import solvers as solver_lib\n",
    "from swirl_dynamics.projects import probabilistic_diffusion as dfn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid tf to use GPU memory\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U-O2msbGzEx"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IpRYEJtGD-Q"
   },
   "outputs": [],
   "source": [
    "# Parameters for the computational task.\n",
    "\n",
    "L = 4 # number of levels (even number)\n",
    "s = 5 # leaf size\n",
    "r = 3 # rank\n",
    "\n",
    "# Discretization of Omega (n_eta * n_eta).\n",
    "neta = (2**L)*s\n",
    "\n",
    "# Number of sources/detectors (n_sc).\n",
    "# Discretization of the domain of alpha in polar coordinates (n_theta * n_rho).\n",
    "# For simplicity, these values are set equal (n_sc = n_theta = n_rho), facilitating computation.\n",
    "nx = (2**L)*s\n",
    "\n",
    "# Standard deviation for the Gaussian blur.\n",
    "blur_sigma = 0.5\n",
    "\n",
    "# Number of training datapoints.\n",
    "NTRAIN = 21000\n",
    "\n",
    "# Number of testing datapoints.\n",
    "NTEST = 500\n",
    "\n",
    "# Total number\n",
    "NTOTAL = NTRAIN + NTEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '../data/10hsquares_trainingdata'\n",
    "\n",
    "# Loading and preprocessing perturbation data (eta)\n",
    "with h5py.File(f'{name}/eta.h5', 'r') as f:\n",
    "    # Read eta data, apply Gaussian blur, and reshape\n",
    "    eta_re = f[list(f.keys())[0]][:NTRAIN, :].reshape(-1, neta, neta)\n",
    "    blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "    eta_re = np.stack([blur_fn(eta_re[i, :, :].T) for i in range(NTRAIN)]).astype('float32')\n",
    "   \n",
    "mean_eta = np.mean(eta_re, axis = 0)\n",
    "eta_re -= mean_eta\n",
    "std_eta = np.std(eta_re)\n",
    "eta_re /= std_eta\n",
    "\n",
    "# Loading and preprocessing scatter data (Lambda)\n",
    "with h5py.File(f'{name}/scatter.h5', 'r') as f:\n",
    "    keys = natsort.natsorted(f.keys())\n",
    "\n",
    "    # Process real part of scatter data\n",
    "    tmp1 = f[keys[3]][:NTRAIN, :]\n",
    "    tmp2 = f[keys[4]][:NTRAIN, :]\n",
    "    tmp3 = f[keys[5]][:NTRAIN, :]\n",
    "    scatter_re = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "\n",
    "    # Process imaginary part of scatter data\n",
    "    tmp1 = f[keys[0]][:NTRAIN, :]\n",
    "    tmp2 = f[keys[1]][:NTRAIN, :]\n",
    "    tmp3 = f[keys[2]][:NTRAIN, :]\n",
    "    scatter_im = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "    \n",
    "    # Combine real and imaginary parts\n",
    "    scatter = np.stack((scatter_re, scatter_im), axis=-2).astype('float32')\n",
    "\n",
    "mean0, std0 = np.mean(scatter[:,:,:,0]), np.std(scatter[:,:,:,0])\n",
    "mean1, std1 = np.mean(scatter[:,:,:,1]), np.std(scatter[:,:,:,1])\n",
    "mean2, std2 = np.mean(scatter[:,:,:,2]), np.std(scatter[:,:,:,2])\n",
    "\n",
    "scatter[:,:,:,0] -= mean0\n",
    "scatter[:,:,:,0] /= std0\n",
    "scatter[:,:,:,1] -= mean1\n",
    "scatter[:,:,:,1] /= std1\n",
    "scatter[:,:,:,2] -= mean2\n",
    "scatter[:,:,:,2] /= std2\n",
    "\n",
    "# Clean up temporary variables to free memory\n",
    "del scatter_re, scatter_im, tmp1, tmp2, tmp3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_train = eta_re.reshape(-1, 80, 80, 1)\n",
    "scatter_train = scatter.reshape(-1, 6400, 2, 3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dict_data = {\"x\": eta_train}\n",
    "dict_data[\"cond\"] = {\"channel:scatter0\": scatter_train[:,:,:,0],\n",
    "                     \"channel:scatter1\": scatter_train[:,:,:,1],\n",
    "                     \"channel:scatter2\": scatter_train[:,:,:,2]}\n",
    "dataset = tf.data.Dataset.from_tensor_slices(dict_data)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "dataset = dataset.as_numpy_iterator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yOBMiJtG7r3"
   },
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZNUY5kQG9xd"
   },
   "source": [
    "The architecture is similar to the unconditional case. We provide additional args that specify how to resize the conditioning signal (in order to be compatible with the noisy sample for channel-wise concatenation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_index = utils.rotationindex(nx)\n",
    "cart_mat = utils.SparsePolarToCartesian(neta, nx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of NN approximations of back-scattering operator for each frequency \n",
    "# n_freq can be changed based on how many frequencies the data has.\n",
    "n_freq = 3\n",
    "\n",
    "fstarlist = [fstars.equinet_fstar( \n",
    "    nx = nx, \n",
    "    neta = neta,\n",
    "    cart_mat = cart_mat,\n",
    "    r_index = r_index\n",
    ") for i in range(n_freq)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_denoiser_model = fstar_cnn.PreconditionedDenoiser(\n",
    "    fstars=fstarlist,\n",
    "    out_channels=1,\n",
    "    squeeze_ratio=8,\n",
    "    cond_embed_iter=10, \n",
    "    noise_embed_dim=96, \n",
    "    num_conv=8,\n",
    "    num_feature=96, # multiples of 32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJFKb060GiRH"
   },
   "outputs": [],
   "source": [
    "diffusion_scheme = dfn_lib.Diffusion.create_variance_preserving(\n",
    "    sigma=dfn_lib.tangent_noise_schedule(),\n",
    "    data_std=1, # we always use normalized data\n",
    ")\n",
    "\n",
    "cond_model = dfn.DenoisingModel(\n",
    "    input_shape=(80,80,1),\n",
    "    cond_shape={\"channel:scatter0\": (6400,2),\n",
    "                \"channel:scatter1\": (6400,2),\n",
    "                \"channel:scatter2\": (6400,2)},\n",
    "    denoiser=cond_denoiser_model,\n",
    "    noise_sampling=dfn_lib.time_uniform_sampling(\n",
    "        diffusion_scheme, clip_min=1e-4, uniform_grid=True,\n",
    "    ),\n",
    "    noise_weighting=dfn_lib.edm_weighting(data_std=1),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekXD8PprGiM8"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "num_train_steps = 21000 * epochs // 16  #@param\n",
    "cond_workdir = os.path.abspath('') + \"/tmp/equinet_cnn_10hsquares\"\n",
    "initial_lr = 1e-5 #@param\n",
    "peak_lr = 1e-3 #@pawram\n",
    "warmup_steps = num_train_steps // 20  #@param\n",
    "end_lr = 1e-8 #@param\n",
    "ema_decay = 0.999  #@param\n",
    "ckpt_interval = 2000 #@param\n",
    "max_ckpt_to_keep = 3 #@param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_params(params, parent_name=''):\n",
    "    \"\"\" Recursively count the number of parameters in the JAX model. \"\"\"\n",
    "    total_params = 0\n",
    "    for key, value in params.items():\n",
    "        layer_name = f\"{parent_name}/{key}\" if parent_name else key\n",
    "        if isinstance(value, dict):\n",
    "            # Recurse into nested dictionary\n",
    "            layer_params = count_params(value, layer_name)\n",
    "            total_params += layer_params\n",
    "        else:\n",
    "            # Assume value is a parameter array\n",
    "            layer_params = value.size\n",
    "            total_params += layer_params\n",
    "            print(f\"Layer: {layer_name}, Parameters: {layer_params}\")\n",
    "    return total_params\n",
    "\n",
    "rng = jax.random.PRNGKey(888)\n",
    "params = cond_model.initialize(rng)\n",
    "total_parameters = count_params(params)\n",
    "print(f\"Total parameters in the model: {total_parameters}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19oJrFsjHCIZ"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DDpmV-zGiKW"
   },
   "outputs": [],
   "source": [
    "cond_trainer = dfn.DenoisingTrainer(\n",
    "    model=cond_model,\n",
    "    rng=jax.random.PRNGKey(888),\n",
    "    optimizer=optax.adam(\n",
    "        learning_rate=optax.warmup_cosine_decay_schedule(\n",
    "            init_value=initial_lr,\n",
    "            peak_value=peak_lr,\n",
    "            warmup_steps=warmup_steps,\n",
    "            decay_steps=num_train_steps,\n",
    "            end_value=end_lr,\n",
    "        ),\n",
    "    ),\n",
    "    ema_decay=ema_decay,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates.run_train(\n",
    "    train_dataloader=dataset,\n",
    "    trainer=cond_trainer,\n",
    "    workdir=cond_workdir,\n",
    "    total_train_steps=num_train_steps,\n",
    "    metric_writer=metric_writers.create_default_writer(\n",
    "        cond_workdir, asynchronous=False\n",
    "    ),\n",
    "    metric_aggregation_steps = 100,\n",
    "    callbacks=(\n",
    "        templates.TqdmProgressBar(\n",
    "            total_train_steps=num_train_steps,\n",
    "            train_monitors=(\"train_loss\",),\n",
    "        ),\n",
    "        templates.TrainStateCheckpoint(\n",
    "            base_dir=cond_workdir,\n",
    "            options=ocp.CheckpointManagerOptions( \n",
    "                save_interval_steps=ckpt_interval, max_to_keep=max_ckpt_to_keep\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AS0m_f0CHR5i"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RHlke6pGiHx"
   },
   "outputs": [],
   "source": [
    "trained_state = dfn.DenoisingModelTrainState.restore_from_orbax_ckpt(\n",
    "    f\"{cond_workdir}/checkpoints\", step=None\n",
    ")\n",
    "\n",
    "# Construc\"\"t the inference function\n",
    "cond_denoise_fn = dfn.DenoisingTrainer.inference_fn_from_state_dict(\n",
    "    trained_state, use_ema=True, denoiser=cond_denoiser_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnaFzOjOHOu4"
   },
   "outputs": [],
   "source": [
    "cond_sampler = dfn_lib.SdeSampler(\n",
    "    input_shape=(80,80,1),\n",
    "    integrator=solver_lib.EulerMaruyama(),\n",
    "    tspan=dfn_lib.exponential_noise_decay(diffusion_scheme, num_steps=256, end_sigma=1e-3,),\n",
    "    scheme=diffusion_scheme,\n",
    "    denoise_fn=cond_denoise_fn,\n",
    "    guidance_transforms=(),\n",
    "    apply_denoise_at_end=True,\n",
    "    return_full_paths=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lYF1OUEHZFM"
   },
   "source": [
    "We JIT the generate function for the sake of faster repeated sampling calls. Here we employ `functools.partial` to specify `num_samples_per_cond`, making it easier to vectorize across the batch dimension with `jax.vmap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mT_rLzdgHOsm"
   },
   "outputs": [],
   "source": [
    "num_samples_per_cond = 50 # Choose the number of samples for each condition\n",
    "\n",
    "generate = jax.jit(\n",
    "    functools.partial(cond_sampler.generate, num_samples_per_cond)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTEST = 500 # Choose the number test points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '../data/10hsquares_testdata'\n",
    "\n",
    "# Loading and preprocessing perturbation data (eta)\n",
    "with h5py.File(f'{name}/eta.h5', 'r') as f:\n",
    "    # Read eta data, apply Gaussian blur, and reshape\n",
    "    eta_re = f[list(f.keys())[0]][:NTEST, :].reshape(-1, neta, neta)\n",
    "    blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "    eta_re = np.stack([blur_fn(img.T) for img in eta_re]).astype('float32')\n",
    "\n",
    "# Loading and preprocessing scatter data (Lambda)\n",
    "with h5py.File(f'{name}/scatter_order_8.h5', 'r') as f:\n",
    "    keys = natsort.natsorted(f.keys())\n",
    "\n",
    "    # Process real part of scatter data\n",
    "    tmp1 = f[keys[3]][:NTEST, :]\n",
    "    tmp2 = f[keys[4]][:NTEST, :]\n",
    "    tmp3 = f[keys[5]][:NTEST, :]\n",
    "    scatter_re = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "\n",
    "    # Process imaginary part of scatter data\n",
    "    tmp1 = f[keys[0]][:NTEST, :]\n",
    "    tmp2 = f[keys[1]][:NTEST, :]\n",
    "    tmp3 = f[keys[2]][:NTEST, :]\n",
    "    scatter_im = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "    \n",
    "    # Combine real and imaginary parts\n",
    "    scatter = np.stack((scatter_re, scatter_im), axis=-2).astype('float32')\n",
    "    \n",
    "scatter[:,:,:,0] -= mean0\n",
    "scatter[:,:,:,0] /= std0\n",
    "scatter[:,:,:,1] -= mean1\n",
    "scatter[:,:,:,1] /= std1\n",
    "scatter[:,:,:,2] -= mean2\n",
    "scatter[:,:,:,2] /= std2\n",
    "\n",
    "# Clean up temporary variables to free memory\n",
    "del scatter_re, scatter_im, tmp1, tmp2, tmp3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_test = eta_re.reshape(-1, 80, 80, 1)\n",
    "scatter_test = scatter.reshape(-1, 6400, 2, 3)\n",
    "c = 0.0 # percentage of noise to add\n",
    "scatter_test += np.random.normal(0, c, size=scatter_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_test = 10\n",
    "dict_data_test = {}\n",
    "dict_data_test[\"cond\"] = {\"channel:scatter0\": scatter_test[:,:,:,0],\n",
    "                          \"channel:scatter1\": scatter_test[:,:,:,1],\n",
    "                          \"channel:scatter2\": scatter_test[:,:,:,2]}\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices(dict_data_test)\n",
    "dataset_test = dataset_test.batch(batch_size_test)\n",
    "dataset_test = dataset_test.prefetch(tf.data.AUTOTUNE)\n",
    "dataset_test = dataset_test.as_numpy_iterator()\n",
    "\n",
    "eta_pred = np.zeros((NTEST, num_samples_per_cond, neta, neta, 1))\n",
    "\n",
    "b = 0\n",
    "for batch in dataset_test:\n",
    "    print(f\"\\rProcessing batch {b + 1} / {NTEST//batch_size_test}\", end='', flush=True)\n",
    "    cond_samples = jax.device_get(jax.vmap(generate, in_axes=(0, 0, None))(\n",
    "        jax.random.split(jax.random.PRNGKey(68), batch_size_test),\n",
    "        batch[\"cond\"],\n",
    "        None,  # Guidance inputs = None since no guidance transforms involved  \n",
    "    ))\n",
    "    eta_pred[b*batch_size_test:(b+1)*batch_size_test,:,:,:,:] = cond_samples*std_eta+mean_eta[:, :, jnp.newaxis]\n",
    "    b += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in range(NTEST):\n",
    "    for j in range(num_samples_per_cond):\n",
    "        errors.append(np.linalg.norm(eta_test[i,:,:,0]-eta_pred[i,0,:,:,0])/np.linalg.norm(eta_test[i,:,:,0]))\n",
    "        \n",
    "print('Mean of validation relative l2 error:', np.mean(errors))\n",
    "print('Median of validation relative l2 error:', np.median(errors))\n",
    "print('Min of validation relative l2 error:', np.min(errors))\n",
    "print('Max of validation relative l2 error:', np.max(errors))\n",
    "print('Standard deviation of validation relative l2 errors:', np.std(errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with h5py.File(\"results_equinet_cnn_squares.h5\", \"w\") as f:\n",
    "#    f.create_dataset('eta', data=eta_test)\n",
    "#    f.create_dataset('eta_pred', data=eta_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "A100",
   "last_runtime": {
    "build_target": "//learning/grp/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1eA8hF0r-tUgIX-miyPgPkzH80WjzCarp",
     "timestamp": 1707268348992
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
