{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP6GQNwnCrwz"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the current working directory to the \"src\" folder, located one level up from the current directory.\n",
    "# Note: This command can only be run once in a Jupyter notebook, as it permanently changes the working directory.\n",
    "%cd ../src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDKhSAGaCrk2"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import fstars \n",
    "import fstar_cnn\n",
    "\n",
    "import functools\n",
    "import os\n",
    "from clu import metric_writers\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "import h5py\n",
    "import natsort\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import geometric_transform\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from swirl_dynamics import templates\n",
    "from swirl_dynamics.lib import diffusion as dfn_lib\n",
    "from swirl_dynamics.lib import solvers as solver_lib\n",
    "from swirl_dynamics.projects import probabilistic_diffusion as dfn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid tf to use GPU memory\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U-O2msbGzEx"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IpRYEJtGD-Q"
   },
   "outputs": [],
   "source": [
    "# Parameters for the computational task.\n",
    "\n",
    "L = 4 # number of levels (even number)\n",
    "s = 5 # leaf size\n",
    "r = 3 # rank\n",
    "\n",
    "# Discretization of Omega (n_eta * n_eta).\n",
    "neta = (2**L)*s\n",
    "\n",
    "# Number of sources/detectors (n_sc).\n",
    "# Discretization of the domain of alpha in polar coordinates (n_theta * n_rho).\n",
    "# For simplicity, these values are set equal (n_sc = n_theta = n_rho), facilitating computation.\n",
    "nx = (2**L)*s\n",
    "\n",
    "# Standard deviation for the Gaussian blur.\n",
    "blur_sigma = 0.5\n",
    "\n",
    "# Number of training datapoints.\n",
    "NTRAIN = 21000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '../data/10hsquares_trainingdata'\n",
    "\n",
    "# Loading and preprocessing perturbation data (eta)\n",
    "with h5py.File(f'{name}/eta.h5', 'r') as f:\n",
    "    # Read eta data, apply Gaussian blur, and reshape\n",
    "    eta_re = f[list(f.keys())[0]][:NTRAIN, :].reshape(-1, neta, neta)\n",
    "    blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "    eta_re = np.stack([blur_fn(eta_re[i, :, :].T) for i in range(NTRAIN)]).astype('float32')\n",
    "    \n",
    "mean_eta = np.mean(eta_re, axis = 0)\n",
    "eta_re -= mean_eta\n",
    "std_eta = np.std(eta_re)\n",
    "eta_re /= std_eta\n",
    "\n",
    "# Loading and preprocessing scatter data (Lambda)\n",
    "with h5py.File(f'{name}/scatter.h5', 'r') as f:\n",
    "    keys = natsort.natsorted(f.keys())\n",
    "\n",
    "    # Process real part of scatter data\n",
    "    tmp1 = f[keys[3]][:NTRAIN, :]\n",
    "    tmp2 = f[keys[4]][:NTRAIN, :]\n",
    "    tmp3 = f[keys[5]][:NTRAIN, :]\n",
    "    scatter_re = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "\n",
    "    # Process imaginary part of scatter data\n",
    "    tmp1 = f[keys[0]][:NTRAIN, :]\n",
    "    tmp2 = f[keys[1]][:NTRAIN, :]\n",
    "    tmp3 = f[keys[2]][:NTRAIN, :]\n",
    "    scatter_im = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "    \n",
    "    # Combine real and imaginary parts\n",
    "    scatter = np.stack((scatter_re, scatter_im), axis=-2).astype('float32')\n",
    "\n",
    "mean0, std0 = np.mean(scatter[:,:,:,0]), np.std(scatter[:,:,:,0])\n",
    "mean1, std1 = np.mean(scatter[:,:,:,1]), np.std(scatter[:,:,:,1])\n",
    "mean2, std2 = np.mean(scatter[:,:,:,2]), np.std(scatter[:,:,:,2])\n",
    "\n",
    "scatter[:,:,:,0] -= mean0\n",
    "scatter[:,:,:,0] /= std0\n",
    "scatter[:,:,:,1] -= mean1\n",
    "scatter[:,:,:,1] /= std1\n",
    "scatter[:,:,:,2] -= mean2\n",
    "scatter[:,:,:,2] /= std2\n",
    "\n",
    "# Clean up temporary variables to free memory\n",
    "del scatter_re, scatter_im, tmp1, tmp2, tmp3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_train = eta_re.reshape(-1, 80, 80, 1)\n",
    "scatter_train = np.swapaxes(scatter.reshape(-1, 80, 80, 2, 3),1,2).reshape(-1, 6400, 2, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dict_data = {\"x\": eta_train}\n",
    "dict_data[\"cond\"] = {\"channel:scatter0\": scatter_train[:,:,:,0],\n",
    "                     \"channel:scatter1\": scatter_train[:,:,:,1],\n",
    "                     \"channel:scatter2\": scatter_train[:,:,:,2]}\n",
    "dataset = tf.data.Dataset.from_tensor_slices(dict_data)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "dataset = dataset.as_numpy_iterator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yOBMiJtG7r3"
   },
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZNUY5kQG9xd"
   },
   "source": [
    "The architecture is similar to the unconditional case. We provide additional args that specify how to resize the conditioning signal (in order to be compatible with the noisy sample for channel-wise concatenation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing analytical back-scattering operator\n",
    "F_adj_3, F_adj_5, F_adj_10 = utils.compute_F_adj(2.5), utils.compute_F_adj(5), utils.compute_F_adj(10)\n",
    "fstarlist = [fstars.analytical_fstar(F_adj_3), fstars.analytical_fstar(F_adj_5), fstars.analytical_fstar(F_adj_10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_denoiser_model = fstar_cnn.PreconditionedDenoiser(\n",
    "    fstars=fstarlist,\n",
    "    out_channels=1,\n",
    "    squeeze_ratio=8,\n",
    "    cond_embed_iter=10, \n",
    "    noise_embed_dim=96, \n",
    "    num_conv=8,\n",
    "    num_feature=96, # multiples of 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJFKb060GiRH"
   },
   "outputs": [],
   "source": [
    "diffusion_scheme = dfn_lib.Diffusion.create_variance_preserving(\n",
    "    sigma=dfn_lib.tangent_noise_schedule(),\n",
    "    data_std=1, # we always use normalized data\n",
    ")\n",
    "\n",
    "cond_model = dfn.DenoisingModel(\n",
    "    input_shape=(80,80,1),\n",
    "    cond_shape={\"channel:scatter0\": (6400,2),\n",
    "                \"channel:scatter1\": (6400,2),\n",
    "                \"channel:scatter2\": (6400,2)},\n",
    "    denoiser=cond_denoiser_model,\n",
    "    noise_sampling=dfn_lib.time_uniform_sampling(\n",
    "        diffusion_scheme, clip_min=1e-4, uniform_grid=True,\n",
    "    ),\n",
    "    noise_weighting=dfn_lib.edm_weighting(data_std=1),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekXD8PprGiM8"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "num_train_steps = 21000 * epochs // 16  #@param\n",
    "cond_workdir = os.path.abspath('') + \"/tmp/analytical_cnn_10hsquares\"\n",
    "initial_lr = 1e-5 #@param\n",
    "peak_lr = 5e-4 #@pawram\n",
    "warmup_steps = num_train_steps // 20  #@param\n",
    "end_lr = 1e-8 #@param\n",
    "ema_decay = 0.999  #@param\n",
    "ckpt_interval = 2000 #@param\n",
    "max_ckpt_to_keep = 3 #@param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_params(params, parent_name=''):\n",
    "    \"\"\" Recursively count the number of parameters in the JAX model. \"\"\"\n",
    "    total_params = 0\n",
    "    for key, value in params.items():\n",
    "        layer_name = f\"{parent_name}/{key}\" if parent_name else key\n",
    "        if isinstance(value, dict):\n",
    "            # Recurse into nested dictionary\n",
    "            layer_params = count_params(value, layer_name)\n",
    "            total_params += layer_params\n",
    "        else:\n",
    "            # Assume value is a parameter array\n",
    "            layer_params = value.size\n",
    "            total_params += layer_params\n",
    "            print(f\"Layer: {layer_name}, Parameters: {layer_params}\")\n",
    "    return total_params\n",
    "    \n",
    "rng = jax.random.PRNGKey(888)\n",
    "params = cond_model.initialize(rng)\n",
    "total_parameters = count_params(params)\n",
    "print(f\"Total parameters in the model: {total_parameters}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19oJrFsjHCIZ"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DDpmV-zGiKW"
   },
   "outputs": [],
   "source": [
    "cond_trainer = dfn.DenoisingTrainer(\n",
    "    model=cond_model,\n",
    "    rng=jax.random.PRNGKey(888),\n",
    "    optimizer=optax.adam(\n",
    "        learning_rate=optax.warmup_cosine_decay_schedule(\n",
    "            init_value=initial_lr,\n",
    "            peak_value=peak_lr,\n",
    "            warmup_steps=warmup_steps,\n",
    "            decay_steps=num_train_steps,\n",
    "            end_value=end_lr,\n",
    "        ),\n",
    "    ),\n",
    "    ema_decay=ema_decay,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates.run_train(\n",
    "    train_dataloader=dataset,\n",
    "    trainer=cond_trainer,\n",
    "    workdir=cond_workdir,\n",
    "    total_train_steps=num_train_steps,\n",
    "    metric_writer=metric_writers.create_default_writer(\n",
    "        cond_workdir, asynchronous=False\n",
    "    ),\n",
    "    metric_aggregation_steps = 100,\n",
    "    callbacks=(\n",
    "        templates.TqdmProgressBar(\n",
    "            total_train_steps=num_train_steps,\n",
    "            train_monitors=(\"train_loss\",),\n",
    "        ),\n",
    "        templates.TrainStateCheckpoint(\n",
    "            base_dir=cond_workdir,\n",
    "            options=ocp.CheckpointManagerOptions( \n",
    "                save_interval_steps=ckpt_interval, max_to_keep=max_ckpt_to_keep\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AS0m_f0CHR5i"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RHlke6pGiHx"
   },
   "outputs": [],
   "source": [
    "trained_state = dfn.DenoisingModelTrainState.restore_from_orbax_ckpt(\n",
    "    f\"{cond_workdir}/checkpoints\", step=None\n",
    ")\n",
    "\n",
    "# Construct the inference function\n",
    "cond_denoise_fn = dfn.DenoisingTrainer.inference_fn_from_state_dict(\n",
    "    trained_state, use_ema=True, denoiser=cond_denoiser_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnaFzOjOHOu4"
   },
   "outputs": [],
   "source": [
    "cond_sampler = dfn_lib.SdeSampler(\n",
    "    input_shape=(80,80,1),\n",
    "    integrator=solver_lib.EulerMaruyama(),\n",
    "    tspan=dfn_lib.exponential_noise_decay(diffusion_scheme, num_steps=256, end_sigma=1e-3,),\n",
    "    scheme=diffusion_scheme,\n",
    "    denoise_fn=cond_denoise_fn,\n",
    "    guidance_transforms=(),\n",
    "    apply_denoise_at_end=True,\n",
    "    return_full_paths=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lYF1OUEHZFM"
   },
   "source": [
    "We again JIT the generate function for the sake of faster repeated sampling calls. Here we employ `functools.partial` to specify `num_samples=5`, making it easier to vectorize across the batch dimension with `jax.vmap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mT_rLzdgHOsm"
   },
   "outputs": [],
   "source": [
    "num_samples_per_cond = 50 # Choose the number of samples for each condition\n",
    "\n",
    "generate = jax.jit(\n",
    "    functools.partial(cond_sampler.generate, num_samples_per_cond)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTEST = 500 # Choose the number test points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_9TfCMSHd3P"
   },
   "source": [
    "Loading a test batch of conditions with 4 elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '../data/10hsquares_testdata'\n",
    "\n",
    "# Loading and preprocessing perturbation data (eta)\n",
    "with h5py.File(f'{name}/eta.h5', 'r') as f:\n",
    "    # Read eta data, apply Gaussian blur, and reshape\n",
    "    eta_re = f[list(f.keys())[0]][:NTEST, :].reshape(-1, neta, neta)\n",
    "    blur_fn = lambda x: gaussian_filter(x, sigma=blur_sigma)\n",
    "    eta_re = np.stack([blur_fn(img.T) for img in eta_re]).astype('float32')\n",
    "\n",
    "# Loading and preprocessing scatter data (Lambda)\n",
    "with h5py.File(f'{name}/scatter_order_8.h5', 'r') as f:\n",
    "    keys = natsort.natsorted(f.keys())\n",
    "\n",
    "    # Process real part of scatter data\n",
    "    tmp1 = f[keys[3]][:NTEST, :]\n",
    "    tmp2 = f[keys[4]][:NTEST, :]\n",
    "    tmp3 = f[keys[5]][:NTEST, :]\n",
    "    scatter_re = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "\n",
    "    # Process imaginary part of scatter data\n",
    "    tmp1 = f[keys[0]][:NTEST, :]\n",
    "    tmp2 = f[keys[1]][:NTEST, :]\n",
    "    tmp3 = f[keys[2]][:NTEST, :]\n",
    "    scatter_im = np.stack((tmp1, tmp2, tmp3), axis=-1)\n",
    "    \n",
    "    # Combine real and imaginary parts\n",
    "    scatter = np.stack((scatter_re, scatter_im), axis=-2).astype('float32')\n",
    "    \n",
    "scatter[:,:,:,0] -= mean0\n",
    "scatter[:,:,:,0] /= std0\n",
    "scatter[:,:,:,1] -= mean1\n",
    "scatter[:,:,:,1] /= std1\n",
    "scatter[:,:,:,2] -= mean2\n",
    "scatter[:,:,:,2] /= std2\n",
    "\n",
    "# Clean up temporary variables to free memory\n",
    "del scatter_re, scatter_im, tmp1, tmp2, tmp3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_test = eta_re.reshape(-1, 80, 80, 1)\n",
    "scatter_test = np.swapaxes(scatter.reshape(-1, 80, 80, 2, 3),1,2).reshape(-1, 6400, 2, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_test = 10\n",
    "dict_data_test = {}\n",
    "dict_data_test[\"cond\"] = {\"channel:scatter0\": scatter_test[:,:,:,0],\n",
    "                          \"channel:scatter1\": scatter_test[:,:,:,1],\n",
    "                          \"channel:scatter2\": scatter_test[:,:,:,2]}\n",
    "\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices(dict_data_test)\n",
    "dataset_test = dataset_test.batch(batch_size_test)\n",
    "dataset_test = dataset_test.prefetch(tf.data.AUTOTUNE)\n",
    "dataset_test = dataset_test.as_numpy_iterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eta_pred = np.zeros((NTEST, num_samples_per_cond, neta, neta, 1))\n",
    "\n",
    "b = 0\n",
    "for batch in dataset_test:\n",
    "    print(b)\n",
    "    cond_samples = jax.device_get(jax.vmap(generate, in_axes=(0, 0, None))(\n",
    "        jax.random.split(jax.random.PRNGKey(888), batch_size_test),\n",
    "        batch[\"cond\"],\n",
    "        None,  # Guidance inputs = None since no guidance transforms involved\n",
    "    ))\n",
    "    eta_pred[b*batch_size_test:(b+1)*batch_size_test,:,:,:,:] = cond_samples*std_eta+mean_eta[:, :, jnp.newaxis]\n",
    "    b += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in range(NTEST):\n",
    "    for j in range(num_samples_per_cond):\n",
    "        errors.append(np.linalg.norm(eta_test[i,:,:,0]-eta_pred[i,j,:,:,0])/np.linalg.norm(eta_test[i,:,:,0]))\n",
    "        \n",
    "print('Mean of validation relative l2 error:', np.mean(errors))\n",
    "print('Median of validation relative l2 error:', np.median(errors))\n",
    "print('Min of validation relative l2 error:', np.min(errors))\n",
    "print('Max of validation relative l2 error:', np.max(errors))\n",
    "print('Standard deviation of validation relative l2 errors:', np.std(errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with h5py.File(\"results_analytical_cnn_squares.h5\", \"w\") as f:\n",
    "#    f.create_dataset('eta', data=eta_test)\n",
    "#    f.create_dataset('eta_pred', data=eta_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "A100",
   "last_runtime": {
    "build_target": "//learning/grp/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1eA8hF0r-tUgIX-miyPgPkzH80WjzCarp",
     "timestamp": 1707268348992
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
