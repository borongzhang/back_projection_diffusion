{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP6GQNwnCrwz"
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/borongzhang/back_projection_diffusion.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDKhSAGaCrk2"
   },
   "outputs": [],
   "source": [
    "from back_projection_diffusion.src import utils, fstars, fstar_cnn\n",
    "\n",
    "import functools\n",
    "import os\n",
    "from clu import metric_writers\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "import h5py\n",
    "import natsort\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import geometric_transform\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from swirl_dynamics import templates\n",
    "from swirl_dynamics.lib import diffusion as dfn_lib\n",
    "from swirl_dynamics.lib iport solvers as solver_lib\n",
    "from swirl_dynamics.projects import probabilistic_diffusion as dfn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid tf to use GPU memory\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U-O2msbGzEx"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IpRYEJtGD-Q"
   },
   "outputs": [],
   "source": [
    "# Parameters for the computational task.\n",
    "\n",
    "L = 4 # number of levels (even number)\n",
    "s = 5 # leaf size\n",
    "r = 3 # rank\n",
    "\n",
    "# Discretization of Omega (n_eta * n_eta).\n",
    "neta = (2**L)*s\n",
    "\n",
    "# Number of sources/detectors (n_sc).\n",
    "# Discretization of the domain of alpha in polar coordinates (n_theta * n_rho).\n",
    "# For simplicity, these values are set equal (n_sc = n_theta = n_rho), facilitating computation.\n",
    "nx = (2**L)*s\n",
    "\n",
    "# Standard deviation for the Gaussian blur.\n",
    "blur_sigma = 0.5\n",
    "\n",
    "# Number of training datapoints.\n",
    "ntrain = 21000\n",
    "batch_size_train = 16\n",
    "\n",
    "# Number of testing datapoints.\n",
    "ntest = 500\n",
    "batch_size_test = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_eta_path = os.path.abspath('../..') + '/data/10hsquares_trainingdata/eta.h5'\n",
    "training_scatter_path = os.path.abspath('../..') + '/data/10hsquares_trainingdata/scatter.h5'\n",
    "eta_train, mean_eta, std_eta = utils.load_eta_data(training_eta_path, ntrain, blur_sigma=0.5, normalize=True)\n",
    "scatter_train, norm_constants = utils.load_scatter_data(training_scatter_path, ntrain, scatter_norm_constants=None)\n",
    "\n",
    "test_eta_path = os.path.abspath('../..') + '/data/10hsquares_testdata/eta.h5'\n",
    "test_scatter_path = os.path.abspath('../..') + '/data/10hsquares_testdata/scatter_order_8.h5'\n",
    "eta_test = utils.load_eta_data(test_eta_path, ntest, blur_sigma=0.5, normalize=False)\n",
    "scatter_test = utils.load_scatter_data(test_scatter_path, ntest, scatter_norm_constants=norm_constants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_train = eta_train.reshape(-1, 80, 80, 1)\n",
    "scatter_train = scatter_train.reshape(-1, 6400, 2, 3) \n",
    "dataset = utils.create_dataset(eta_train, scatter_train, batch_size=batch_size_train, repeat=True)\n",
    "\n",
    "eta_test = eta_test.reshape(-1, 80, 80, 1)\n",
    "scatter_test = scatter_test.reshape(-1, 6400, 2, 3)\n",
    "c = 0.0 # percentage of noise to add\n",
    "scatter_test += np.random.normal(0, c, size = scatter_test.shape)\n",
    "dataset_test = utils.create_dataset(eta_test, scatter_test, batch_size=batch_size_test, repeat=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yOBMiJtG7r3"
   },
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = nx\n",
    "L2x = nx\n",
    "L2y = nx\n",
    "Nw1 = 20\n",
    "Nb1 = L1 // Nw1\n",
    "Nw2x = 10\n",
    "Nw2y = 10\n",
    "Nb2x = L2x // Nw2x\n",
    "Nb2y = L2y // Nw2y\n",
    "r = 3  # rank \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of NN approximations of the back-scattering operator for each frequency \n",
    "# n_freq can be changed based on how many frequencies the data has.\n",
    "n_freq = 3\n",
    "\n",
    "fstarlist = [fstars.SwitchNetFstar( \n",
    "    L1=L1, L2x=L2x, L2y=L2y, Nw1=Nw1, Nb1=Nb1, \n",
    "    Nw2x=Nw2x, Nw2y=Nw2y, Nb2x=Nb2x, Nb2y=Nb2y, \n",
    "    r=r\n",
    ") for i in range(n_freq)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_denoiser_model = fstar_cnn.PreconditionedDenoiser(\n",
    "    fstars=fstarlist,\n",
    "    out_channels=1,\n",
    "    squeeze_ratio=8,\n",
    "    cond_embed_iter=10, \n",
    "    noise_embed_dim=96, \n",
    "    num_conv=8,\n",
    "    num_feature=96, # multiples of 32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJFKb060GiRH"
   },
   "outputs": [],
   "source": [
    "diffusion_scheme = dfn_lib.Diffusion.create_variance_preserving(\n",
    "    sigma=dfn_lib.tangent_noise_schedule(),\n",
    "    data_std=1, # we always use normalized data\n",
    ")\n",
    "\n",
    "cond_model = dfn.DenoisingModel(\n",
    "    input_shape=(80,80,1),\n",
    "    cond_shape={\"channel:scatter0\": (6400,2),\n",
    "                \"channel:scatter1\": (6400,2),\n",
    "                \"channel:scatter2\": (6400,2)},\n",
    "    denoiser=cond_denoiser_model,\n",
    "    noise_sampling=dfn_lib.time_uniform_sampling(\n",
    "        diffusion_scheme, clip_min=1e-4, uniform_grid=True,\n",
    "    ),\n",
    "    noise_weighting=dfn_lib.edm_weighting(data_std=1),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(888)\n",
    "params = cond_model.initialize(rng)\n",
    "total_parameters = utils.count_params(params)\n",
    "print(f\"Total parameters in the model: {total_parameters}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19oJrFsjHCIZ"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekXD8PprGiM8"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "num_train_steps = 21000 * epochs // 16  #@param\n",
    "cond_workdir = os.path.abspath('..') + \"/tmp/switchnet_cnn_10hsquares\"\n",
    "initial_lr = 1e-5 #@param\n",
    "peak_lr = 1e-3 #@pawram\n",
    "warmup_steps = num_train_steps // 20  #@param\n",
    "end_lr = 1e-8 #@param\n",
    "ema_decay = 0.999  #@param\n",
    "ckpt_interval = 2000 #@param\n",
    "max_ckpt_to_keep = 3 #@param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DDpmV-zGiKW"
   },
   "outputs": [],
   "source": [
    "cond_trainer = dfn.DenoisingTrainer(\n",
    "    model=cond_model,\n",
    "    rng=jax.random.PRNGKey(888),\n",
    "    optimizer=optax.adam(\n",
    "        learning_rate=optax.warmup_cosine_decay_schedule(\n",
    "            init_value=initial_lr,\n",
    "            peak_value=peak_lr,\n",
    "            warmup_steps=warmup_steps,\n",
    "            decay_steps=num_train_steps,\n",
    "            end_value=end_lr,\n",
    "        ),\n",
    "    ),\n",
    "    ema_decay=ema_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates.run_train(\n",
    "    train_dataloader=dataset,\n",
    "    trainer=cond_trainer,\n",
    "    workdir=cond_workdir,\n",
    "    total_train_steps=num_train_steps,\n",
    "    metric_writer=metric_writers.create_default_writer(\n",
    "        cond_workdir, asynchronous=False\n",
    "    ),\n",
    "    metric_aggregation_steps = 100,\n",
    "    callbacks=(\n",
    "        templates.TqdmProgressBar(\n",
    "            total_train_steps=num_train_steps,\n",
    "            train_monitors=(\"train_loss\",),\n",
    "        ),\n",
    "        templates.TrainStateCheckpoint(\n",
    "            base_dir=cond_workdir,\n",
    "            options=ocp.CheckpointManagerOptions( \n",
    "                save_interval_steps=ckpt_interval, max_to_keep=max_ckpt_to_keep\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AS0m_f0CHR5i"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RHlke6pGiHx"
   },
   "outputs": [],
   "source": [
    "trained_state = dfn.DenoisingModelTrainState.restore_from_orbax_ckpt(\n",
    "    f\"{cond_workdir}/checkpoints\", step=None\n",
    ")\n",
    "\n",
    "# Construct the inference function\n",
    "cond_denoise_fn = dfn.DenoisingTrainer.inference_fn_from_state_dict(\n",
    "    trained_state, use_ema=True, denoiser=cond_denoiser_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnaFzOjOHOu4"
   },
   "outputs": [],
   "source": [
    "cond_sampler = dfn_lib.SdeSampler(\n",
    "    input_shape=(80,80,1),\n",
    "    integrator=solver_lib.EulerMaruyama(),\n",
    "    tspan=dfn_lib.exponential_noise_decay(diffusion_scheme, num_steps=256, end_sigma=1e-3),\n",
    "    scheme=diffusion_scheme,\n",
    "    denoise_fn=cond_denoise_fn,\n",
    "    guidance_transforms=(),\n",
    "    apply_denoise_at_end=True,\n",
    "    return_full_paths=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mT_rLzdgHOsm"
   },
   "outputs": [],
   "source": [
    "num_samples_per_cond = 10\n",
    "\n",
    "generate = jax.jit(\n",
    "    functools.partial(cond_sampler.generate, num_samples_per_cond)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_pred = np.zeros((ntest, num_samples_per_cond, neta, neta, 1))\n",
    "\n",
    "b = 0\n",
    "for batch in dataset_test:\n",
    "    print(f\"\\rProcessing batch {b + 1} / {ntest//batch_size_test}\", end='', flush=True)\n",
    "    cond_samples = jax.device_get(jax.vmap(generate, in_axes=(0, 0, None))(\n",
    "        jax.random.split(jax.random.PRNGKey(68), batch_size_test),\n",
    "        batch[\"cond\"],\n",
    "        None,  # Guidance inputs = None since no guidance transforms involved  \n",
    "    ))\n",
    "    eta_pred[b*batch_size_test:(b+1)*batch_size_test,:,:,:,:] = cond_samples*std_eta+mean_eta[:, :, jnp.newaxis]\n",
    "    b += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in range(ntest):\n",
    "    errors.append(np.linalg.norm(eta_test[i,:,:,0]-eta_pred[i,0,:,:,0])/np.linalg.norm(eta_test[i,:,:,0]))\n",
    "        \n",
    "print('Mean of validation relative l2 error:', np.mean(errors))\n",
    "print('Median of validation relative l2 error:', np.median(errors))\n",
    "print('Min of validation relative l2 error:', np.min(errors))\n",
    "print('Max of validation relative l2 error:', np.max(errors))\n",
    "print('Standard deviation of validation relative l2 errors:', np.std(errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with h5py.File(\"results_switchnet_cnn_10hsquares.h5\", \"w\") as f:\n",
    "#    f.create_dataset('eta', data=eta_test)\n",
    "#    f.create_dataset('eta_pred', data=eta_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "A100",
   "last_runtime": {
    "build_target": "//learning/grp/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1eA8hF0r-tUgIX-miyPgPkzH80WjzCarp",
     "timestamp": 1707268348992
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
